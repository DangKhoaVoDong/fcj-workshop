[{"uri":"https://dangkhoavodong.github.io/fcj-workshop/3-blogstranslated/3.1-blog1/","title":"Blog 1","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, “Getting Started with Healthcare Data Lakes: Diving into Amazon Cognito”, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the “pub/sub hub.”\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function → ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda “trigger” subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 → JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://dangkhoavodong.github.io/fcj-workshop/3-blogstranslated/3.2-blog2/","title":"Blog 2","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, “Getting Started with Healthcare Data Lakes: Diving into Amazon Cognito”, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the “pub/sub hub.”\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function → ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda “trigger” subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 → JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://dangkhoavodong.github.io/fcj-workshop/3-blogstranslated/3.3-blog3/","title":"Blog 3","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, “Getting Started with Healthcare Data Lakes: Diving into Amazon Cognito”, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the “pub/sub hub.”\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function → ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda “trigger” subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 → JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://dangkhoavodong.github.io/fcj-workshop/3-blogstranslated/3.4-blog4/","title":"Blog 4","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, “Getting Started with Healthcare Data Lakes: Diving into Amazon Cognito”, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the “pub/sub hub.”\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function → ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda “trigger” subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 → JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://dangkhoavodong.github.io/fcj-workshop/3-blogstranslated/3.5-blog5/","title":"Blog 5","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, “Getting Started with Healthcare Data Lakes: Diving into Amazon Cognito”, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the “pub/sub hub.”\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function → ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda “trigger” subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 → JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://dangkhoavodong.github.io/fcj-workshop/3-blogstranslated/3.6-blog6/","title":"Blog 6","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, “Getting Started with Healthcare Data Lakes: Diving into Amazon Cognito”, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the “pub/sub hub.”\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function → ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda “trigger” subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 → JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://dangkhoavodong.github.io/fcj-workshop/5-workshop/5.3-s3-vpc/5.3.1-create-gwe/","title":"Create a gateway endpoint","tags":[],"description":"","content":" Open the Amazon VPC console In the navigation pane, choose Endpoints, then click Create Endpoint: You will see 6 existing VPC endpoints that support AWS Systems Manager (SSM). These endpoints were deployed automatically by the CloudFormation Templates for this workshop.\nIn the Create endpoint console: Specify name of the endpoint: s3-gwe In service category, choose AWS services In Services, type s3 in the search box and choose the service with type gateway For VPC, select VPC Cloud from the drop-down. For Configure route tables, select the route table that is already associated with two subnets (note: this is not the main route table for the VPC, but a second route table created by CloudFormation). For Policy, leave the default option, Full Access, to allow full access to the service. You will deploy a VPC endpoint policy in a later lab module to demonstrate restricting access to S3 buckets based on policies. Do not add a tag to the VPC endpoint at this time. Click Create endpoint, then click x after receiving a successful creation message. "},{"uri":"https://dangkhoavodong.github.io/fcj-workshop/4-eventparticipated/4.1-event1/","title":"Event 1","tags":[],"description":"","content":"Event Report: “Kick-off AWS First Cloud Journey Workforce OJT FALL 2025” Event Objectives Officially kick off the AWS First Cloud Journey – Training on the Job (OJT Fall 2025) internship program. Provide career orientation for students in Cloud Computing, DevOps, and AI fields. Connect students with experts from AWS, partner enterprises, and the Alumni community. Inspire and instill the \u0026ldquo;AWS Builders\u0026rdquo; spirit in the future generation of engineers. List of Speakers Mr. Nguyen Tran Phuoc Bao - Head of Enterprise Relations Department. Mr. Nguyen Gia Hung - Head of Solutions Architect, AWS Vietnam. Mr. Do Huy Thang - DevOps Lead, VNG. Mr. Danh Hoang Hieu Nghi - GenAI Engineer, Renova (Alumni). Ms. Bui Ho Linh Nhi - AI Engineer, SoftwareOne (Alumni). Mr. Pham Nguyen Hai Anh - Cloud Engineer, G-Asia Pacific (Alumni). Mr. Nguyen Dong Thanh Hiep - Principal Cloud Engineer, G-Asia Pacific (Alumni). Key Highlights Journey Kick-off \u0026amp; Strategic Vision A Major Milestone: The event marks the transition from an academic environment to real-world enterprise experience (OJT). Training Goals: Building a high-quality workforce equipped with combat-ready skills in Cloud, DevOps, AI/ML, Security, and Data Analytics. Bridge of Knowledge: The program is not just about technology training but also connects students with the AWS Study Group community (47,000+ members) and major enterprises. Career Orientation: Cloud \u0026amp; DevOps Speakers Nguyen Gia Hung and Do Huy Thang shared the industry landscape:\nMarket Demand: The strong migration of businesses to the Cloud creates huge opportunities for Solutions Architect and DevOps Engineer roles. DevOps Mindset: It is not just about tools; DevOps is a culture that prioritizes collaboration, automation, and efficiency measurement. Development Roadmap: From foundational knowledge (Networking, Linux) to AWS certifications (Practitioner, Associate) and practical experience. Real-world Sharing from Alumni The panel discussion with former students provided authentic perspectives:\nFrom First Cloud Journey to GenAI Engineer: The journey of self-study, overcoming initial difficulties to master new technologies like Generative AI. She in Tech: An inspiring story from a female AI engineer, affirming that women can shine in the high-tech engineering field. A Day in the Life of a Cloud Engineer: Describing actual work, project pressures, and the joy of solving difficult problems for customers. Key Learnings Career Mindset Proactive Learning: Cloud technology changes daily; a spirit of self-study and proactively seeking documentation is vital. Networking is Key: Relationships with mentors and the community are valuable assets that help solve problems faster and open up job opportunities. Resilience: Being ready to face failure and \u0026ldquo;debug\u0026rdquo; mistakes as an inevitable part of the growth process. Industry Knowledge Understood the distinctions and connections between roles: Cloud Engineer, DevOps Engineer, and AI Engineer. Grasped new technology trends being sought after by enterprises: GenAI, Serverless, Automation. Recognized the importance of AWS certifications in proving competency (AWS Certified Solutions Architect, SysOps Administrator). Application to Work Build a Learning Roadmap: Create a specific plan to achieve the AWS Cloud Practitioner certification during the internship. Join the Community: Join the AWS Study Group Vietnam to update knowledge and participate in discussions. Hone Soft Skills: Focus on improving communication and teamwork skills through upcoming practical projects. Focus on Core Tech: Spend time diving deep into core AWS services (EC2, S3, VPC, Lambda) to prepare for specialized workshops. Event Experience Participating in the Kick-off AWS First Cloud Journey at the Bitexco Financial Tower was an exciting and professional experience.\nProfessional and Dynamic Atmosphere Organized in the professional space of Bitexco Financial Tower, the event brought a sense of excitement right from the check-in and reception. The presence of many students and experts created a lively yet open academic atmosphere. Meeting and Connecting Directly listening to and asking questions to \u0026ldquo;Key persons\u0026rdquo; in the industry, like Mr. Gia Hung (AWS) or Mr. Huy Thang (VNG), helped broaden my perspective on industry standards. The networking session with Alumni was the emotional highlight, helping me see an image of myself in the future and believe that this roadmap is entirely feasible. Inspiration from \u0026ldquo;She in Tech\u0026rdquo; Ms. Linh Nhi\u0026rsquo;s story broke gender stereotypes in the engineering industry, providing great motivation for female participants in the program. Key Takeaways The event was not just an opening ceremony but the first lesson in professional attitude and commitment to the chosen path. I realized that behind the speakers\u0026rsquo; success are thousands of hours of effort, and the upcoming OJT journey is when I start accumulating my own \u0026ldquo;flight hours\u0026rdquo;. Event Photos Insert check-in photos, photos with speakers, or the hall overview here * Overall, the Kick-off event equipped me with not only information but also strong motivation to begin the journey of becoming a true AWS Builder.\n"},{"uri":"https://dangkhoavodong.github.io/fcj-workshop/","title":"Internship Report","tags":[],"description":"","content":"Internship Report Student Information: Full Name: Vo Dong Dang Khoa\nPhone Number: 0938453798\nEmail: khoavddse184607@fpt.edu.vn\nUniversity: FPT University\nMajor: Information Technology\nClass: AWS082025\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 08/09/2025 to 08/12/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "},{"uri":"https://dangkhoavodong.github.io/fcj-workshop/5-workshop/5.1-workshop-overview/","title":"Introduction","tags":[],"description":"","content":"VPC endpoints VPC endpoints are virtual devices. They are horizontally scaled, redundant, and highly available VPC components. They allow communication between your compute resources and AWS services without imposing availability risks. Compute resources running in VPC can access Amazon S3 using a Gateway endpoint. PrivateLink interface endpoints can be used by compute resources running in VPC or on-premises. Workshop overview In this workshop, you will use two VPCs.\n\u0026ldquo;VPC Cloud\u0026rdquo; is for cloud resources such as a Gateway endpoint and an EC2 instance to test with. \u0026ldquo;VPC On-Prem\u0026rdquo; simulates an on-premises environment such as a factory or corporate datacenter. An EC2 instance running strongSwan VPN software has been deployed in \u0026ldquo;VPC On-prem\u0026rdquo; and automatically configured to establish a Site-to-Site VPN tunnel with AWS Transit Gateway. This VPN simulates connectivity from an on-premises location to the AWS cloud. To minimize costs, only one VPN instance is provisioned to support this workshop. When planning VPN connectivity for your production workloads, AWS recommends using multiple VPN devices for high availability. "},{"uri":"https://dangkhoavodong.github.io/fcj-workshop/5-workshop/5.4-s3-onprem/5.4.1-prepare/","title":"Prepare the environment","tags":[],"description":"","content":"To prepare for this part of the workshop you will need to:\nDeploying a CloudFormation stack Modifying a VPC route table. These components work together to simulate on-premises DNS forwarding and name resolution.\nDeploy the CloudFormation stack The CloudFormation template will create additional services to support an on-premises simulation:\nOne Route 53 Private Hosted Zone that hosts Alias records for the PrivateLink S3 endpoint One Route 53 Inbound Resolver endpoint that enables \u0026ldquo;VPC Cloud\u0026rdquo; to resolve inbound DNS resolution requests to the Private Hosted Zone One Route 53 Outbound Resolver endpoint that enables \u0026ldquo;VPC On-prem\u0026rdquo; to forward DNS requests for S3 to \u0026ldquo;VPC Cloud\u0026rdquo; Click the following link to open the AWS CloudFormation console. The required template will be pre-loaded into the menu. Accept all default and click Create stack. It may take a few minutes for stack deployment to complete. You can continue with the next step without waiting for the deployemnt to finish.\nUpdate on-premise private route table This workshop uses a strongSwan VPN running on an EC2 instance to simulate connectivty between an on-premises datacenter and the AWS cloud. Most of the required components are provisioned before your start. To finalize the VPN configuration, you will modify the \u0026ldquo;VPC On-prem\u0026rdquo; routing table to direct traffic destined for the cloud to the strongSwan VPN instance.\nOpen the Amazon EC2 console\nSelect the instance named infra-vpngw-test. From the Details tab, copy the Instance ID and paste this into your text editor\nNavigate to the VPC menu by using the Search box at the top of the browser window.\nClick on Route Tables, select the RT Private On-prem route table, select the Routes tab, and click Edit Routes.\nClick Add route. Destination: your Cloud VPC cidr range Target: ID of your infra-vpngw-test instance (you saved in your editor at step 1) Click Save changes "},{"uri":"https://dangkhoavodong.github.io/fcj-workshop/1-worklog/1.1-week1/","title":"Week 1 Worklog","tags":[],"description":"","content":"Week 1 Objectives: Get acquainted with the working environment and First Cloud Journey team members. Learn basic knowledge about AWS Cloud Computing. Create AWS account and familiarize with Console interface. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 7 - Attended orientation session with FCJ members 06/08/2025 06/08/2025 AWS Journey 2 - Read and understood organization\u0026rsquo;s rules and regulations - Learned about the internship program 09/08/2025 09/08/2025 AWS Journey 3-5 - Researched AWS Cloud Platform overview - Read documentation about Cloud Computing and basic concepts - Watched introduction videos about AWS 09/09/2025 09/11/2025 AWS Journey 6 - Created AWS Free Tier account - Completed payment verification - Logged in and explored AWS Management Console interface 09/12/2025 09/12/2025 AWS Journey Week 1 Achievements: Completed orientation and understood FCJ work processes:\nGot acquainted with team members Clearly understood organization\u0026rsquo;s rules and regulations Grasped the learning roadmap for the next 3 months Acquired basic Cloud Computing knowledge:\nUnderstand what Cloud Computing is and service models (IaaS, PaaS, SaaS) Know the benefits of using cloud Learned about AWS and its position in the cloud market Successfully created AWS account:\nRegistered AWS Free Tier account Completed payment verification (credit card) Successfully logged into AWS Management Console Familiarized with basic Console interface "},{"uri":"https://dangkhoavodong.github.io/fcj-workshop/1-worklog/","title":"Worklog","tags":[],"description":"","content":"This is my worklog during 13 weeks of internship at First Cloud Journey, from September 8, 2025 to December 5, 2025. During this time, I learned and practiced with AWS services, from basic to advanced, and completed a comprehensive final project.\nProgram Overview: Duration: 13 weeks (09/08/2025 - 12/05/2025) Objective: Master AWS core services and be able to design and deploy applications on AWS Result: Completed the program with a comprehensive project and ready for AWS certification Weekly Contents: Week 1: Getting familiar with AWS and creating account\nWeek 2: Learning basic AWS services, AWS CLI and EC2\nWeek 3: Advanced practice with EC2, EBS, Elastic IP and S3\nWeek 4: Advanced S3 and IAM - AWS Security\nWeek 5: VPC - Virtual Private Cloud and Networking\nWeek 6: Load Balancing and RDS Database\nWeek 7: Auto Scaling and CloudWatch Monitoring\nWeek 8: Lambda Serverless and CloudFormation IaC\nWeek 9: Route 53 DNS and CloudFront CDN\nWeek 10: DynamoDB NoSQL and SNS/SQS Messaging\nWeek 11: ECS Container Services and AWS Best Practices\nWeek 12: Building Comprehensive Project\nWeek 13: Completing Documentation and Presentation\n"},{"uri":"https://dangkhoavodong.github.io/fcj-workshop/5-workshop/5.4-s3-onprem/5.4.2-create-interface-enpoint/","title":"Create an S3 Interface endpoint","tags":[],"description":"","content":"In this section you will create and test an S3 interface endpoint using the simulated on-premises environment deployed as part of this workshop.\nReturn to the Amazon VPC menu. In the navigation pane, choose Endpoints, then click Create Endpoint.\nIn Create endpoint console:\nName the interface endpoint In Service category, choose aws services In the Search box, type S3 and press Enter. Select the endpoint named com.amazonaws.us-east-1.s3. Ensure that the Type column indicates Interface. For VPC, select VPC Cloud from the drop-down. Make sure to choose \u0026ldquo;VPC Cloud\u0026rdquo; and not \u0026ldquo;VPC On-prem\u0026rdquo;\nExpand Additional settings and ensure that Enable DNS name is not selected (we will use this in the next part of the workshop) Select 2 subnets in the following AZs: us-east-1a and us-east-1b For Security group, choose SGforS3Endpoint: Keep the default policy - full access and click Create endpoint Congratulation on successfully creating S3 interface endpoint. In the next step, we will test the interface endpoint.\n"},{"uri":"https://dangkhoavodong.github.io/fcj-workshop/4-eventparticipated/4.2-event2/","title":"Event 2","tags":[],"description":"","content":"Event Report: “AWS First Cloud Journey Community Day 2025” Event Objectives Create a playground to connect the AWS First Cloud Journey community, gathering students, experts, and businesses. Share real-world use cases regarding Cloud Computing, specifically focusing on emerging trends like GenAI, Serverless, and Multi-Agent Systems. Introduce Reference Architectures in specific sectors such as FinTech, Banking, eCommerce, and Security. List of Speakers The event gathered a large number of speakers who are experts from AWS and talented technical teams:\nMr. Nguyen Gia Hung - Head of Solutions Architect, AWS Vietnam (Keynote Speaker). Mr. Hai Tran - Speaker on Banking/FinTech. Mr. Nguyen Viet Phap - Speaker on eCommerce Scaling. Parallel Session Speakers: Dinh Le Hoang Anh, Nguyen Hong Nhung, Le Minh Nghia, Tran Doan Cong Ly, Kiet Lam, Nguyen Ngoc Quynh Mai, and many other experts. Key Highlights The event covered many in-depth topics:\n1. GenAI \u0026amp; Chatbots in Enterprise (Enterprise Software \u0026amp; FinTech) Enterprise Chatbot with MCP: Introduction to building enterprise chatbots with advanced context handling using the MCP protocol on AWS. Stock Trading Chatbot: Live demo of a stock trading chatbot using Serverless architecture, ensuring real-time processing and high scalability. 2. Applying GenAI to Life \u0026amp; eCommerce GenAI Kitchen Recipe Recommendation: A personalized recipe recommendation system powered by GenAI, illustrating workflow design on AWS. Scaling eCommerce: Leveraging GenAI to optimize customer experience and personalize shopping, while solving the performance scaling challenge. 3. Automation with AI Agents (Automation \u0026amp; Audit) Multi-Agent Systems: Applying multi-agent systems to automate complex banking processes. Auto Audit Framework: Using AI to automate auditing processes, orchestrating workflows between agents to minimize human error. Amazon Nova Act: Introduction to the new generation AI Agents and Cloud-native solutions for the banking sector. 4. Security \u0026amp; Internal Tools Internal Chatbot with RAG: Building an internal knowledge base chatbot using RAG (Retrieval-Augmented Generation) techniques on a fully Serverless platform. Smart Data Contracts: Applying GenAI to build and validate smart data contracts, serving Fintech scenarios and large enterprises. Key Learnings Technical Knowledge RAG \u0026amp; Serverless: Gained a deeper understanding of combining RAG with Serverless architecture to build low-cost but highly effective chatbots. Multi-Agent Architecture: Grasped the concept and power of coordinating multiple AI Agents to solve complex tasks instead of using a single model. Cloud-Native Mindset: Clearly saw the shift towards Cloud-native architectures in traditional industries like Banking and Auditing. Problem-Solving Mindset Technology does not exist in isolation but is always attached to a specific Business Case, such as optimizing trading, personalizing shopping experiences, or automating audits. Application to Work Research RAG: Dive deeper into RAG architecture to propose building an internal documentation search tool for the team during the internship. Test AI Agents: Start exploring Amazon Bedrock Agents or frameworks to build a simple automation task. Reference Architecture: Save the shared reference architecture models to apply when designing systems in the future. Event Experience Participating in the AWS First Cloud Journey Community Day at Bitexco was an \u0026ldquo;overwhelming\u0026rdquo; experience in a positive way due to the massive amount of knowledge shared.\nVibrant Academic Atmosphere The Live Demos of the Chatbot or Auto Audit were impressive, helping me clearly visualize the transition from theory to reality. The Q\u0026amp;A sessions were lively, with speakers openly sharing lessons learned and challenges, not just successes. Quality Networking The Coffee Break was a great opportunity to connect with passionate students from other universities and interact with the speakers. Key Takeaways The event helped me realize that GenAI is no longer just a \u0026ldquo;buzzword\u0026rdquo; but has penetrated deep into enterprise business logic. To avoid falling behind, continuous updates on AI Agents and Automation are mandatory. Event Photos Insert check-in photos at Bitexco, photos of presentation slides, or photos with speakers here * Overall: Community Day was a bountiful \u0026ldquo;tech feast,\u0026rdquo; bringing many fresh ideas and motivation for me to research deeper into GenAI on AWS.\n"},{"uri":"https://dangkhoavodong.github.io/fcj-workshop/5-workshop/5.2-prerequiste/","title":"Prerequiste","tags":[],"description":"","content":"IAM permissions Add the following IAM permission policy to your user account to deploy and cleanup this workshop.\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;VisualEditor0\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;cloudformation:*\u0026#34;, \u0026#34;cloudwatch:*\u0026#34;, \u0026#34;ec2:AcceptTransitGatewayPeeringAttachment\u0026#34;, \u0026#34;ec2:AcceptTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:AllocateAddress\u0026#34;, \u0026#34;ec2:AssociateAddress\u0026#34;, \u0026#34;ec2:AssociateIamInstanceProfile\u0026#34;, \u0026#34;ec2:AssociateRouteTable\u0026#34;, \u0026#34;ec2:AssociateSubnetCidrBlock\u0026#34;, \u0026#34;ec2:AssociateTransitGatewayRouteTable\u0026#34;, \u0026#34;ec2:AssociateVpcCidrBlock\u0026#34;, \u0026#34;ec2:AttachInternetGateway\u0026#34;, \u0026#34;ec2:AttachNetworkInterface\u0026#34;, \u0026#34;ec2:AttachVolume\u0026#34;, \u0026#34;ec2:AttachVpnGateway\u0026#34;, \u0026#34;ec2:AuthorizeSecurityGroupEgress\u0026#34;, \u0026#34;ec2:AuthorizeSecurityGroupIngress\u0026#34;, \u0026#34;ec2:CreateClientVpnEndpoint\u0026#34;, \u0026#34;ec2:CreateClientVpnRoute\u0026#34;, \u0026#34;ec2:CreateCustomerGateway\u0026#34;, \u0026#34;ec2:CreateDhcpOptions\u0026#34;, \u0026#34;ec2:CreateFlowLogs\u0026#34;, \u0026#34;ec2:CreateInternetGateway\u0026#34;, \u0026#34;ec2:CreateLaunchTemplate\u0026#34;, \u0026#34;ec2:CreateNetworkAcl\u0026#34;, \u0026#34;ec2:CreateNetworkInterface\u0026#34;, \u0026#34;ec2:CreateNetworkInterfacePermission\u0026#34;, \u0026#34;ec2:CreateRoute\u0026#34;, \u0026#34;ec2:CreateRouteTable\u0026#34;, \u0026#34;ec2:CreateSecurityGroup\u0026#34;, \u0026#34;ec2:CreateSubnet\u0026#34;, \u0026#34;ec2:CreateSubnetCidrReservation\u0026#34;, \u0026#34;ec2:CreateTags\u0026#34;, \u0026#34;ec2:CreateTransitGateway\u0026#34;, \u0026#34;ec2:CreateTransitGatewayPeeringAttachment\u0026#34;, \u0026#34;ec2:CreateTransitGatewayPrefixListReference\u0026#34;, \u0026#34;ec2:CreateTransitGatewayRoute\u0026#34;, \u0026#34;ec2:CreateTransitGatewayRouteTable\u0026#34;, \u0026#34;ec2:CreateTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:CreateVpc\u0026#34;, \u0026#34;ec2:CreateVpcEndpoint\u0026#34;, \u0026#34;ec2:CreateVpcEndpointConnectionNotification\u0026#34;, \u0026#34;ec2:CreateVpcEndpointServiceConfiguration\u0026#34;, \u0026#34;ec2:CreateVpnConnection\u0026#34;, \u0026#34;ec2:CreateVpnConnectionRoute\u0026#34;, \u0026#34;ec2:CreateVpnGateway\u0026#34;, \u0026#34;ec2:DeleteCustomerGateway\u0026#34;, \u0026#34;ec2:DeleteFlowLogs\u0026#34;, \u0026#34;ec2:DeleteInternetGateway\u0026#34;, \u0026#34;ec2:DeleteNetworkInterface\u0026#34;, \u0026#34;ec2:DeleteNetworkInterfacePermission\u0026#34;, \u0026#34;ec2:DeleteRoute\u0026#34;, \u0026#34;ec2:DeleteRouteTable\u0026#34;, \u0026#34;ec2:DeleteSecurityGroup\u0026#34;, \u0026#34;ec2:DeleteSubnet\u0026#34;, \u0026#34;ec2:DeleteSubnetCidrReservation\u0026#34;, \u0026#34;ec2:DeleteTags\u0026#34;, \u0026#34;ec2:DeleteTransitGateway\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayPeeringAttachment\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayPrefixListReference\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayRoute\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayRouteTable\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:DeleteVpc\u0026#34;, \u0026#34;ec2:DeleteVpcEndpoints\u0026#34;, \u0026#34;ec2:DeleteVpcEndpointServiceConfigurations\u0026#34;, \u0026#34;ec2:DeleteVpnConnection\u0026#34;, \u0026#34;ec2:DeleteVpnConnectionRoute\u0026#34;, \u0026#34;ec2:Describe*\u0026#34;, \u0026#34;ec2:DetachInternetGateway\u0026#34;, \u0026#34;ec2:DisassociateAddress\u0026#34;, \u0026#34;ec2:DisassociateRouteTable\u0026#34;, \u0026#34;ec2:GetLaunchTemplateData\u0026#34;, \u0026#34;ec2:GetTransitGatewayAttachmentPropagations\u0026#34;, \u0026#34;ec2:ModifyInstanceAttribute\u0026#34;, \u0026#34;ec2:ModifySecurityGroupRules\u0026#34;, \u0026#34;ec2:ModifyTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:ModifyVpcAttribute\u0026#34;, \u0026#34;ec2:ModifyVpcEndpoint\u0026#34;, \u0026#34;ec2:ReleaseAddress\u0026#34;, \u0026#34;ec2:ReplaceRoute\u0026#34;, \u0026#34;ec2:RevokeSecurityGroupEgress\u0026#34;, \u0026#34;ec2:RevokeSecurityGroupIngress\u0026#34;, \u0026#34;ec2:RunInstances\u0026#34;, \u0026#34;ec2:StartInstances\u0026#34;, \u0026#34;ec2:StopInstances\u0026#34;, \u0026#34;ec2:UpdateSecurityGroupRuleDescriptionsEgress\u0026#34;, \u0026#34;ec2:UpdateSecurityGroupRuleDescriptionsIngress\u0026#34;, \u0026#34;iam:AddRoleToInstanceProfile\u0026#34;, \u0026#34;iam:AttachRolePolicy\u0026#34;, \u0026#34;iam:CreateInstanceProfile\u0026#34;, \u0026#34;iam:CreatePolicy\u0026#34;, \u0026#34;iam:CreateRole\u0026#34;, \u0026#34;iam:DeleteInstanceProfile\u0026#34;, \u0026#34;iam:DeletePolicy\u0026#34;, \u0026#34;iam:DeleteRole\u0026#34;, \u0026#34;iam:DeleteRolePolicy\u0026#34;, \u0026#34;iam:DetachRolePolicy\u0026#34;, \u0026#34;iam:GetInstanceProfile\u0026#34;, \u0026#34;iam:GetPolicy\u0026#34;, \u0026#34;iam:GetRole\u0026#34;, \u0026#34;iam:GetRolePolicy\u0026#34;, \u0026#34;iam:ListPolicyVersions\u0026#34;, \u0026#34;iam:ListRoles\u0026#34;, \u0026#34;iam:PassRole\u0026#34;, \u0026#34;iam:PutRolePolicy\u0026#34;, \u0026#34;iam:RemoveRoleFromInstanceProfile\u0026#34;, \u0026#34;lambda:CreateFunction\u0026#34;, \u0026#34;lambda:DeleteFunction\u0026#34;, \u0026#34;lambda:DeleteLayerVersion\u0026#34;, \u0026#34;lambda:GetFunction\u0026#34;, \u0026#34;lambda:GetLayerVersion\u0026#34;, \u0026#34;lambda:InvokeFunction\u0026#34;, \u0026#34;lambda:PublishLayerVersion\u0026#34;, \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;logs:DeleteLogGroup\u0026#34;, \u0026#34;logs:DescribeLogGroups\u0026#34;, \u0026#34;logs:PutRetentionPolicy\u0026#34;, \u0026#34;route53:ChangeTagsForResource\u0026#34;, \u0026#34;route53:CreateHealthCheck\u0026#34;, \u0026#34;route53:CreateHostedZone\u0026#34;, \u0026#34;route53:CreateTrafficPolicy\u0026#34;, \u0026#34;route53:DeleteHostedZone\u0026#34;, \u0026#34;route53:DisassociateVPCFromHostedZone\u0026#34;, \u0026#34;route53:GetHostedZone\u0026#34;, \u0026#34;route53:ListHostedZones\u0026#34;, \u0026#34;route53domains:ListDomains\u0026#34;, \u0026#34;route53domains:ListOperations\u0026#34;, \u0026#34;route53domains:ListTagsForDomain\u0026#34;, \u0026#34;route53resolver:AssociateResolverEndpointIpAddress\u0026#34;, \u0026#34;route53resolver:AssociateResolverRule\u0026#34;, \u0026#34;route53resolver:CreateResolverEndpoint\u0026#34;, \u0026#34;route53resolver:CreateResolverRule\u0026#34;, \u0026#34;route53resolver:DeleteResolverEndpoint\u0026#34;, \u0026#34;route53resolver:DeleteResolverRule\u0026#34;, \u0026#34;route53resolver:DisassociateResolverEndpointIpAddress\u0026#34;, \u0026#34;route53resolver:DisassociateResolverRule\u0026#34;, \u0026#34;route53resolver:GetResolverEndpoint\u0026#34;, \u0026#34;route53resolver:GetResolverRule\u0026#34;, \u0026#34;route53resolver:ListResolverEndpointIpAddresses\u0026#34;, \u0026#34;route53resolver:ListResolverEndpoints\u0026#34;, \u0026#34;route53resolver:ListResolverRuleAssociations\u0026#34;, \u0026#34;route53resolver:ListResolverRules\u0026#34;, \u0026#34;route53resolver:ListTagsForResource\u0026#34;, \u0026#34;route53resolver:UpdateResolverEndpoint\u0026#34;, \u0026#34;route53resolver:UpdateResolverRule\u0026#34;, \u0026#34;s3:AbortMultipartUpload\u0026#34;, \u0026#34;s3:CreateBucket\u0026#34;, \u0026#34;s3:DeleteBucket\u0026#34;, \u0026#34;s3:DeleteObject\u0026#34;, \u0026#34;s3:GetAccountPublicAccessBlock\u0026#34;, \u0026#34;s3:GetBucketAcl\u0026#34;, \u0026#34;s3:GetBucketOwnershipControls\u0026#34;, \u0026#34;s3:GetBucketPolicy\u0026#34;, \u0026#34;s3:GetBucketPolicyStatus\u0026#34;, \u0026#34;s3:GetBucketPublicAccessBlock\u0026#34;, \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:GetObjectVersion\u0026#34;, \u0026#34;s3:GetBucketVersioning\u0026#34;, \u0026#34;s3:ListAccessPoints\u0026#34;, \u0026#34;s3:ListAccessPointsForObjectLambda\u0026#34;, \u0026#34;s3:ListAllMyBuckets\u0026#34;, \u0026#34;s3:ListBucket\u0026#34;, \u0026#34;s3:ListBucketMultipartUploads\u0026#34;, \u0026#34;s3:ListBucketVersions\u0026#34;, \u0026#34;s3:ListJobs\u0026#34;, \u0026#34;s3:ListMultipartUploadParts\u0026#34;, \u0026#34;s3:ListMultiRegionAccessPoints\u0026#34;, \u0026#34;s3:ListStorageLensConfigurations\u0026#34;, \u0026#34;s3:PutAccountPublicAccessBlock\u0026#34;, \u0026#34;s3:PutBucketAcl\u0026#34;, \u0026#34;s3:PutBucketPolicy\u0026#34;, \u0026#34;s3:PutBucketPublicAccessBlock\u0026#34;, \u0026#34;s3:PutObject\u0026#34;, \u0026#34;secretsmanager:CreateSecret\u0026#34;, \u0026#34;secretsmanager:DeleteSecret\u0026#34;, \u0026#34;secretsmanager:DescribeSecret\u0026#34;, \u0026#34;secretsmanager:GetSecretValue\u0026#34;, \u0026#34;secretsmanager:ListSecrets\u0026#34;, \u0026#34;secretsmanager:ListSecretVersionIds\u0026#34;, \u0026#34;secretsmanager:PutResourcePolicy\u0026#34;, \u0026#34;secretsmanager:TagResource\u0026#34;, \u0026#34;secretsmanager:UpdateSecret\u0026#34;, \u0026#34;sns:ListTopics\u0026#34;, \u0026#34;ssm:DescribeInstanceProperties\u0026#34;, \u0026#34;ssm:DescribeSessions\u0026#34;, \u0026#34;ssm:GetConnectionStatus\u0026#34;, \u0026#34;ssm:GetParameters\u0026#34;, \u0026#34;ssm:ListAssociations\u0026#34;, \u0026#34;ssm:ResumeSession\u0026#34;, \u0026#34;ssm:StartSession\u0026#34;, \u0026#34;ssm:TerminateSession\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } Provision resources using CloudFormation In this lab, we will use N.Virginia region (us-east-1).\nTo prepare the workshop environment, deploy this CloudFormation Template (click link): PrivateLinkWorkshop . Accept all of the defaults when deploying the template.\nTick 2 acknowledgement boxes Choose Create stack The ClouddFormation deployment requires about 15 minutes to complete.\n2 VPCs have been created 3 EC2s have been created "},{"uri":"https://dangkhoavodong.github.io/fcj-workshop/2-proposal/","title":"Proposal","tags":[],"description":"","content":"Coffee Cloud – Coffee Shop Order Platform Project Documentation 📄 Download Complete Project Proposal (Word Document)\n1. Executive Summary The \u0026ldquo;Coffee Cloud – Coffee Shop Order Platform\u0026rdquo; project is a web platform that helps customers order coffee online, accumulate points after each order, and redeem promotional vouchers.\nThe system supports three user groups: Customer, Shipper, and Admin, aiming to optimize the ordering experience, delivery, and operational management of the coffee shop.\nThe frontend application is built with ReactJS, backend with C#/.NET running on AWS Lambda, connected via API Gateway and deployed entirely on AWS Free Tier with services: Amplify (Hosting + CI/CD), Cognito (Authentication), Lambda .NET (Backend logic), S3 Storage, DynamoDB, SNS (Notifications), SES (Email Service), IAM, and CloudWatch Logs.\n2. Problem Statement What\u0026rsquo;s the Problem? Traditional coffee shops face difficulties managing crowded orders, customers have to wait long to place orders and receive products. There is no point accumulation system to encourage customers to return, and order status tracking is not transparent.\nThe Solution The Coffee Cloud platform is built with scalable cloud architecture on AWS, using ReactJS for frontend hosted on Amplify, and containerized backend services deployed on Elastic Beanstalk. The system provides three separate interfaces for Customer (ordering, point accumulation), Shipper (receiving and delivering), and Admin (overall management). Data is stored in DynamoDB for high performance with S3 for static assets and Amazon Location Service for delivery tracking. Authentication is managed by Cognito with multi-role support. AWS SNS handles real-time notifications about order status, while SES manages email communications. Payment processing is integrated through external payment gateways. Key features include online ordering, point system, real-time order tracking with GPS, inventory management, and multi-channel notification system.\nBenefits and Return on Investment The system helps increase revenue through online channels, reduces customer waiting time, and optimizes operational processes. Low deployment cost thanks to AWS Free Tier usage, estimated monthly operating cost under $5 USD for the initial phase. The point system helps increase customer return rate, expected to increase revenue by 20-30% compared to traditional methods. Return on investment time is estimated at 3-6 months thanks to saving labor costs and increasing sales efficiency.\n3. Solution Architecture Coffee Cloud adopts a scalable containerized architecture on AWS to ensure high availability and cost efficiency. The ReactJS frontend is deployed on AWS Amplify with automatic CI/CD integration from Git repository. Backend services are containerized and deployed on AWS Elastic Beanstalk for easy scaling and management. Data is stored in DynamoDB for high performance, S3 for static assets, and Amazon Location Service for GPS tracking and delivery optimization. Authentication and authorization are managed by Amazon Cognito with multi-role support (Customer, Shipper, Admin). External payment services are integrated for secure transaction processing.\nAWS Services Used AWS Amplify: Hosts ReactJS frontend with automatic CI/CD pipeline from Git repository. AWS Elastic Beanstalk: Containerized .NET Framework 8.0 backend services with auto-scaling and load balancing. Amazon DynamoDB: NoSQL database storing users, orders, products, points data. Amazon S3: Stores static assets like product images, documents, and backup data. Amazon Location Service: GPS tracking, geocoding, and route optimization for delivery. Amazon Cognito: Authentication and authorization for 3 user role types. Amazon SNS: Push notifications and SMS alerts for order status and promotions. Amazon SES: Email service for order confirmations, receipts, and marketing campaigns. Cloudflare Tunnel: Secure domain connection between Amplify frontend and Elastic Beanstalk backend using Quick Tunnel. Payment Gateway Integration: External payment processing for secure transactions. Amazon CloudWatch: Monitoring and logging for the entire system. AWS IAM: Manages permissions and security policies. Component Design Frontend Layer: ReactJS application hosted on Amplify with responsive design and Git-based CI/CD. Application Layer: Containerized .NET Framework 8.0 services deployed on Elastic Beanstalk with auto-scaling capabilities. Network Layer: Cloudflare Tunnel (Quick Tunnel) providing secure domain connection between frontend and backend services. Data Storage Layer: DynamoDB for structured data, S3 for file storage, Location Service for GPS data. Authentication Layer: Cognito User Pools manage users with 3 groups (Customer, Shipper, Admin). Communication Layer: SNS for real-time notifications, SES for email communications. Payment Layer: Integrated external payment gateways for secure transaction processing. Monitoring Layer: CloudWatch tracks performance, errors, and usage metrics with IAM security controls. 4. Technical Implementation Implementation Phases The Coffee Cloud project is divided into 4 main phases over 3 months:\nResearch and Design: Analyze business requirements, design database schema, wireframe UI/UX and system architecture (Month 1). Environment Setup and Backend Development: Configure AWS services, develop containerized backend services, set up DynamoDB tables and Elastic Beanstalk environment (Month 1-2). Frontend Development: Build ReactJS application, integrate with backend APIs, implement authentication flow with Cognito (Month 2). Testing and Deployment: Unit testing, integration testing, performance testing, deploy to Amplify and Elastic Beanstalk with monitoring (Month 3). Technical Requirements\nFrontend Requirements: ReactJS with hooks, React Router for navigation, Axios for API calls, CSS frameworks (Bootstrap/Material-UI), responsive design for mobile and desktop. Backend Requirements: Containerized .NET Framework 8.0 services on Elastic Beanstalk, Entity Framework Core for data access, JWT authentication, exception handling and logging. Database Design: DynamoDB tables for Users, Products, Orders, OrderItems, Points, Vouchers with proper indexing and relationships. Location Services: Amazon Location Service integration for GPS tracking, geocoding, and route optimization. Payment Integration: External payment gateway APIs for secure transaction processing. Network Integration: Cloudflare Tunnel (Quick Tunnel) for secure connection between Amplify frontend and Elastic Beanstalk backend. DevOps Requirements: Git version control, Amplify CI/CD pipeline, Elastic Beanstalk deployment, CloudWatch monitoring, IAM roles and policies for security. 5. Timeline \u0026amp; Milestones Project Timeline\nPhase 1 (Week 1-4): System Research and Design Analyze business and technical requirements Design database schema and API specifications Create wireframes and UI mockups Setup AWS account and initial configuration Phase 2 (Week 5-8): Backend and Infrastructure Development Create DynamoDB tables and configure indexes Develop containerized backend services with C#/.NET Setup Elastic Beanstalk environment and deployment Configure Cognito User Pools and Groups Integrate Amazon Location Service for GPS tracking Setup payment gateway integration Phase 3 (Week 9-10): Frontend Development Build ReactJS components and pages Implement authentication and authorization Integrate with backend APIs Responsive design for mobile Phase 4 (Week 11-12): Testing and Deployment Unit testing and integration testing Deploy to AWS Amplify Performance optimization and monitoring setup User acceptance testing and documentation 6. Budget Estimation The Coffee Cloud project is designed to maximize AWS Free Tier usage during the initial development and testing phase.\nInfrastructure Costs AWS Services (Monthly)\nAWS Amplify: $0.00 USD (Free Tier: 1000 build minutes, 15GB storage) AWS Elastic Beanstalk: $0.00 USD (Free Tier: t3.micro instance, 750 hours/month) Amazon DynamoDB: $0.00 USD (Free Tier: 25GB storage, 25 RCU/WCU) Amazon S3: $0.00 USD (Free Tier: 5GB standard storage) Amazon Location Service: $0.00 USD (Free Tier: 5000 requests/month) Amazon Cognito: $0.00 USD (Free Tier: 50,000 MAU) Amazon SNS: $0.00 USD (Free Tier: 1M publications, 100,000 HTTP/HTTPS requests) Amazon SES: $0.00 USD (Free Tier: 62,000 emails/month) Amazon CloudWatch: $0.00 USD (Free Tier: 10 custom metrics, 5GB logs) Post Free Tier Costs (Estimated for production)\nAWS Amplify: ~$1.00 USD/month (hosting + build minutes) AWS Elastic Beanstalk: ~$15.00 USD/month (t3.small instance for production) Amazon DynamoDB: ~$1.25 USD/month (additional RCU/WCU) Amazon Location Service: ~$2.00 USD/month (additional GPS requests) Amazon SNS: ~$0.50 USD/month (additional notifications + SMS) Amazon SES: ~$1.00 USD/month (additional emails beyond free tier) Payment Gateway Fees: ~$5.00 USD/month (transaction processing fees) Total Estimation: $0.00 USD/month (Development), ~$25.75 USD/month (Production) Development Cost: Only incurs developer labor costs\n7. Risk Assessment Risk Matrix Exceeding Free Tier limits: Medium impact, medium probability Integration errors between AWS services: High impact, low probability DynamoDB performance issues: Medium impact, low probability Security vulnerabilities: High impact, low probability Mitigation Strategies Cost: Set up CloudWatch billing alerts, monitor usage daily Integration: Perform thorough testing, use AWS SAM for local testing Performance: Design proper DynamoDB indexes, implement caching strategies Security: Follow AWS security best practices, regular security audits Contingency Plans Backup and recovery plan for DynamoDB data Fallback mechanisms for critical functions Manual operation procedures if system fails Communication plan with stakeholders during incidents 8. Expected Outcomes Technical Improvements Complete Coffee Cloud system capable of handling hundreds of concurrent orders, responsive design working smoothly on all devices.\nBusiness Benefits 30% revenue increase through new online channel 50% reduction in order processing time 25% increase in customer return rate through point system Improve customer satisfaction score to 90% Skill Development Master AWS Serverless Architecture Full-stack development experience with ReactJS and .NET Deep understanding of NoSQL database design DevOps skills with CI/CD pipeline Scalability The system can easily scale to serve multiple coffee shops or integrate additional features like AI recommendations, advanced loyalty programs.\nLong-term Value Platform foundation can be reused for other e-commerce projects, creating a foundation for developing similar business applications.\n"},{"uri":"https://dangkhoavodong.github.io/fcj-workshop/5-workshop/5.3-s3-vpc/5.3.2-test-gwe/","title":"Test the Gateway Endpoint","tags":[],"description":"","content":"Create S3 bucket Navigate to S3 management console In the Bucket console, choose Create bucket In the Create bucket console Name the bucket: choose a name that hasn\u0026rsquo;t been given to any bucket globally (hint: lab number and your name) Leave other fields as they are (default) Scroll down and choose Create bucket Successfully create S3 bucket. Connect to EC2 with session manager For this workshop, you will use AWS Session Manager to access several EC2 instances. Session Manager is a fully managed AWS Systems Manager capability that allows you to manage your Amazon EC2 instances and on-premises virtual machines (VMs) through an interactive one-click browser-based shell. Session Manager provides secure and auditable instance management without the need to open inbound ports, maintain bastion hosts, or manage SSH keys.\nFirst cloud journey Lab for indepth understanding of Session manager.\nIn the AWS Management Console, start typing Systems Manager in the quick search box and press Enter: From the Systems Manager menu, find Node Management in the left menu and click Session Manager: Click Start Session, and select the EC2 instance named Test-Gateway-Endpoint. This EC2 instance is already running in \u0026ldquo;VPC Cloud\u0026rdquo; and will be used to test connectivity to Amazon S3 through the Gateway endpoint you just created (s3-gwe).\nSession Manager will open a new browser tab with a shell prompt: sh-4.2 $\nYou have successfully start a session - connect to the EC2 instance in VPC cloud. In the next step, we will create a S3 bucket and a file in it.\nCreate a file and upload to s3 bucket Change to the ssm-user\u0026rsquo;s home directory by typing cd ~ in the CLI Create a new file to use for testing with the command fallocate -l 1G testfile.xyz, which will create a file of 1GB size named \u0026ldquo;testfile.xyz\u0026rdquo;. Upload file to S3 bucket with command aws s3 cp testfile.xyz s3://your-bucket-name. Replace your-bucket-name with the name of S3 bucket that you created earlier. You have successfully uploaded the file to your S3 bucket. You can now terminate the session.\nCheck object in S3 bucket Navigate to S3 console. Click the name of your s3 bucket In the Bucket console, you will see the file you have uploaded to your S3 bucket Section summary Congratulation on completing access to S3 from VPC. In this section, you created a Gateway endpoint for Amazon S3, and used the AWS CLI to upload an object. The upload worked because the Gateway endpoint allowed communication to S3, without needing an Internet Gateway attached to \u0026ldquo;VPC Cloud\u0026rdquo;. This demonstrates the functionality of the Gateway endpoint as a secure path to S3 without traversing the Public Internet.\n"},{"uri":"https://dangkhoavodong.github.io/fcj-workshop/1-worklog/1.2-week2/","title":"Week 2 Worklog","tags":[],"description":"","content":"Week 2 Objectives: Learn about basic AWS services. Install and use AWS CLI. Start practicing with EC2. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2-3 - Studied 4 main AWS service groups: + Compute (EC2, Lambda) + Storage (S3, EBS, EFS) + Networking (VPC, Route53) + Database (RDS, DynamoDB) - Watched service demos 09/15/2025 09/16/2025 AWS Journey 4 - Installed AWS CLI on computer - Created IAM User and Access Key - Configured AWS CLI (aws configure) 09/17/2025 09/17/2025 AWS Journey 5-6 - Learned about Amazon EC2 basics - Created first EC2 instance (t2.micro) - Practiced SSH connection to EC2 09/18/2025 09/19/2025 AWS Journey Week 2 Achievements: Grasped overview of AWS services:\nUnderstand 4 main service groups and their purposes Know when to use which service Familiarized with AWS Regions and Availability Zones Successfully installed and configured AWS CLI:\nInstalled AWS CLI on Windows Created IAM User with appropriate permissions Configured Access Key and Secret Key Tested basic CLI commands Practiced with Amazon EC2:\nSuccessfully created first EC2 instance Understand Instance types and AMI Configured Security Group Successfully connected to EC2 instance via SSH "},{"uri":"https://dangkhoavodong.github.io/fcj-workshop/5-workshop/5.3-s3-vpc/","title":"Access S3 from VPC","tags":[],"description":"","content":"Using Gateway endpoint In this section, you will create a Gateway eendpoint to access Amazon S3 from an EC2 instance. The Gateway endpoint will allow upload an object to S3 buckets without using the Public Internet. To create an endpoint, you must specify the VPC in which you want to create the endpoint, and the service (in this case, S3) to which you want to establish the connection.\nContent Create gateway endpoint Test gateway endpoint "},{"uri":"https://dangkhoavodong.github.io/fcj-workshop/4-eventparticipated/4.3-event3/","title":"Event 3","tags":[],"description":"","content":"Event Report: “Workshop: AI/ML/GenAI on AWS” Event Objectives Provide a comprehensive overview of the AI/ML landscape and trends in Vietnam. Equip participants with foundational and in-depth knowledge of the AWS AI/ML toolset. Guide through the \u0026ldquo;end-to-end\u0026rdquo; Machine Learning process with Amazon SageMaker. Explore the power of Generative AI through Amazon Bedrock, from Prompt Engineering to building Chatbot applications. List of Speakers Team of experts from AWS Vietnam (AWS Solutions Architects). Key Highlights The event was divided into two main parts, progressing from traditional ML to the most advanced GenAI technologies:\n1. Overview of AWS AI/ML Services \u0026amp; Amazon SageMaker AI Landscape in Vietnam: Discussing opportunities and challenges when deploying AI in the local enterprise environment. Amazon SageMaker – Comprehensive ML Platform: Data Preparation: Data preparation and labeling processes. Training \u0026amp; Tuning: How to train models and fine-tune parameters for maximum accuracy. Deployment \u0026amp; MLOps: Deploying models to production and managing the model lifecycle. Live Demo: Visual experience of the SageMaker Studio interface. 2. Generative AI with Amazon Bedrock Foundation Models (FMs): Comparing the pros and cons of top foundation models like Claude, Llama, and Titan to choose the right fit for the problem. Prompt Engineering: Optimization techniques such as Chain-of-Thought reasoning and Few-shot learning to get the best results from the model. RAG (Retrieval-Augmented Generation): Architecture combining models with private data (Knowledge Base) to minimize hallucinations and increase accuracy. Bedrock Agents: Building multi-step workflows and integrating external tools. Guardrails: Setting up content filters to ensure safety and policy compliance. Key Learnings Technical Knowledge SageMaker vs. Bedrock: Clearly understood when to use SageMaker (to train custom models with specific data) and when to use Bedrock (to leverage the existing power of large Foundation Models). RAG Architecture: Mastered the mechanism of RAG - the key to bringing enterprise data into AI without retraining the model. Responsible AI: Understood the importance of controlling output content using Guardrails. Practical Skills Learned how to design an effective Prompt to control AI behavior as desired. Visualized the data flow in a real-world Chatbot application. Application to Work Solution Proposal: In upcoming projects, if chatbot or text summarization features are needed, I will propose using Amazon Bedrock due to its convenience and fast integration. Self-Study: Dedicate time to practice SageMaker JumpStart labs to get familiar with deploying open-source models. Process Improvement: Apply the MLOps mindset to the software development process for better version control and automation. Event Experience Participating in the AI/ML/GenAI on AWS workshop at the AWS Vietnam office was a very different experience compared to self-learning online.\nProfessional Environment Visiting the AWS office directly helped me feel the professional and dynamic working culture of a world-leading technology company. The workshop space was well-arranged, creating the best conditions for following demos and practicing. Practical \u0026amp; Understandable Demos The Live Demo of building a Generative AI Chatbot using Bedrock was truly impressive. It showed me that integrating AI into applications is no longer as complicated and distant as before. The speakers explained difficult concepts like Chain-of-Thought or RAG in a very visual and easy-to-absorb manner. Networking \u0026amp; Expansion The Ice-breaker session and Coffee Break helped me get to know many like-minded friends and enthusiastic mentors. Key Takeaways The event helped me define my direction more clearly: Focusing on Generative AI Application Development rather than diving too deep into building core models from scratch. Event Photos Insert check-in photos at the AWS office, screenshots of the demo, or the workshop space here * Overall: The workshop equipped me with \u0026ldquo;heavy weaponry\u0026rdquo; regarding AI/ML, making me much more confident when approaching technology projects in the future.\n"},{"uri":"https://dangkhoavodong.github.io/fcj-workshop/5-workshop/5.4-s3-onprem/5.4.3-test-endpoint/","title":"Test the Interface Endpoint","tags":[],"description":"","content":"Get the regional DNS name of S3 interface endpoint From the Amazon VPC menu, choose Endpoints.\nClick the name of newly created endpoint: s3-interface-endpoint. Click details and save the regional DNS name of the endpoint (the first one) to your text-editor for later use.\nConnect to EC2 instance in \u0026ldquo;VPC On-prem\u0026rdquo; Navigate to Session manager by typing \u0026ldquo;session manager\u0026rdquo; in the search box\nClick Start Session, and select the EC2 instance named Test-Interface-Endpoint. This EC2 instance is running in \u0026ldquo;VPC On-prem\u0026rdquo; and will be used to test connectivty to Amazon S3 through the Interface endpoint we just created. Session Manager will open a new browser tab with a shell prompt: sh-4.2 $\nChange to the ssm-user\u0026rsquo;s home directory with command \u0026ldquo;cd ~\u0026rdquo;\nCreate a file named testfile2.xyz\nfallocate -l 1G testfile2.xyz Copy file to the same S3 bucket we created in section 3.2 aws s3 cp --endpoint-url https://bucket.\u0026lt;Regional-DNS-Name\u0026gt; testfile2.xyz s3://\u0026lt;your-bucket-name\u0026gt; This command requires the \u0026ndash;endpoint-url parameter, because you need to use the endpoint-specific DNS name to access S3 using an Interface endpoint. Do not include the leading \u0026rsquo; * \u0026rsquo; when copying/pasting the regional DNS name. Provide your S3 bucket name created earlier Now the file has been added to your S3 bucket. Let check your S3 bucket in the next step.\nCheck Object in S3 bucket Navigate to S3 console Click Buckets Click the name of your bucket and you will see testfile2.xyz has been added to your bucket "},{"uri":"https://dangkhoavodong.github.io/fcj-workshop/3-blogstranslated/","title":"Translated Blogs","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nThis section will list and introduce the blogs you have translated. For example:\nBlog 1 - Getting started with healthcare data lakes: Using microservices This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices…), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 2 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices…), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 3 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices…), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 4 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices…), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 5 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices…), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 6 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices…), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\n"},{"uri":"https://dangkhoavodong.github.io/fcj-workshop/1-worklog/1.3-week3/","title":"Week 3 Worklog","tags":[],"description":"","content":"Week 3 Objectives: Advanced practice with EC2. Learn about EBS and Elastic IP. Start learning about S3. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Practiced managing multiple EC2 instances - Learned about EC2 instance states (start, stop, terminate) 09/22/2025 09/22/2025 AWS Journey 3-4 - Learned about EBS (Elastic Block Store) - Practiced attaching EBS volume to EC2 - Created snapshot and restore 09/23/2025 09/24/2025 AWS Journey 5 - Learned about Elastic IP - Configured Elastic IP for EC2 instance 09/25/2025 09/25/2025 AWS Journey 6 - Learned basics about Amazon S3 - Created first S3 bucket - Uploaded/downloaded files 09/26/2025 09/26/2025 AWS Journey Week 3 Achievements: More proficient with EC2:\nCan manage multiple EC2 instances Understand instance states Know how to start, stop, terminate instances Use tags to manage resources Grasped EBS volumes:\nUnderstand EBS and volume types (gp2, gp3, io1) Successfully attached and detached EBS volume Created snapshot for backup Restored volume from snapshot Configured Elastic IP:\nUnderstand Elastic IP and its purpose Assigned Elastic IP to EC2 instance Know how to manage and release Elastic IP Started working with S3:\nSuccessfully created S3 bucket Uploaded and downloaded files Understand basic S3 storage classes Configured bucket permissions "},{"uri":"https://dangkhoavodong.github.io/fcj-workshop/5-workshop/5.4-s3-onprem/","title":"Access S3 from on-premises","tags":[],"description":"","content":"Overview In this section, you will create an Interface endpoint to access Amazon S3 from a simulated on-premises environment. The Interface endpoint will allow you to route to Amazon S3 over a VPN connection from your simulated on-premises environment.\nWhy using Interface endpoint:\nGateway endpoints only work with resources running in the VPC where they are created. Interface endpoints work with resources running in VPC, and also resources running in on-premises environments. Connectivty from your on-premises environment to the cloud can be provided by AWS Site-to-Site VPN or AWS Direct Connect. Interface endpoints allow you to connect to services powered by AWS PrivateLink. These services include some AWS services, services hosted by other AWS customers and partners in their own VPCs (referred to as PrivateLink Endpoint Services), and supported AWS Marketplace Partner services. For this workshop, we will focus on connecting to Amazon S3. "},{"uri":"https://dangkhoavodong.github.io/fcj-workshop/4-eventparticipated/","title":"Attended Events","tags":[],"description":"","content":"During my internship, I actively participated in a series of specialized events and workshops. Each event was a valuable opportunity to update my knowledge of new technologies and orient my career development.\nEvent 1 Event Name: Kick-off AWS First Cloud Journey Workforce OJT FALL 2025\nTime: 08:30, September 06, 2025\nLocation: Floor 26, Bitexco Financial Tower, 02 Hai Trieu, HCMC\nRole: Attendee\nDescription:\nAttended the kick-off ceremony for the OJT Fall 2025 internship program, listening to career orientation from AWS and VNG experts regarding the Cloud \u0026amp; DevOps career path. Networked with seniors and alumni to learn about their real-world journeys to becoming GenAI Engineers and Cloud Engineers. Key Outcomes:\nUnderstood the development roadmap for AWS Builders and prepared the right mindset for the internship. Expanded connections with the student community and industry mentors. Event 2 Event Name: AWS First Cloud Journey Community Day 2025\nTime: 08:30 - 11:30, August 30, 2025\nLocation: Floors 26 \u0026amp; 36, Bitexco Financial Tower, HCMC\nRole: Attendee\nDescription:\nAttended the opening session on the AWS community vision. Participated in parallel sessions covering diverse practical topics: Enterprise/FinTech: Chatbot applications in stock trading and enterprise management. GenAI \u0026amp; Automation: Recipe recommendation systems, internal Chatbots using RAG, and banking process automation with Multi-Agent Systems. Security: Smart Data Contracts powered by GenAI. Key Outcomes:\nGrasped real-world Use Cases of GenAI and Serverless in the Banking and E-commerce sectors. Gained further understanding of Multi-Agent architecture and how to apply AI to business process automation. Event 3 Event Name: Workshop: AWS Cloud Mastery Series #1\nTime: 08:30 - 12:00, November 15, 2025\nLocation: AWS Vietnam Office\nRole: Attendee\nDescription:\nOverview of the AI/ML landscape in Vietnam and core AWS AI/ML services. Amazon SageMaker: Learned the End-to-end ML process (data preparation, training, deployment) and watched a demo on SageMaker Studio. Generative AI with Amazon Bedrock: Compared Foundation Models (Claude, Llama, Titan), learned Prompt Engineering techniques, RAG architecture, and Knowledge Base integration. Key Outcomes:\nLearned how to use Amazon SageMaker for traditional Machine Learning problems. Deeply understood Generative AI, how to build Chatbots using Bedrock Agents, and ensured content safety with Guardrails. Event 4 Event Name: Workshop: AWS Cloud Mastery Series #2\nTime: 08:30 - 17:00, November 17, 2025 (Full-day)\nLocation: AWS Vietnam Office\nRole: Attendee\nDescription:\nMorning: Learned about DevOps culture, building CI/CD Pipelines (CodeCommit, CodeBuild, CodeDeploy, CodePipeline), and Infrastructure as Code (CloudFormation, CDK). Afternoon: Explored Container services (Docker, ECR, ECS/EKS), deployment strategies (Blue/Green, Canary), and system monitoring (Monitoring \u0026amp; Observability) with CloudWatch, X-Ray. Key Outcomes:\nMastered the CI/CD process and how to automate software development on AWS. Practiced deploying infrastructure as code (IaC) and managing Container applications effectively. Learned Best Practices for monitoring and incident handling. Event 5 Event Name: Workshop: AWS Cloud Mastery Series #3\nTime: 08:30 - 12:00, November 29, 2025\nLocation: AWS Vietnam Office\nRole: Attendee\nDescription:\nDeep dive into the 5 security pillars according to AWS Well-Architected standards: Identity (IAM): Access management, Zero Trust, MFA. Detection: Continuous monitoring with CloudTrail, GuardDuty. Infrastructure Protection: Network protection (VPC, WAF, Shield). Data Protection: Data encryption (KMS) and secrets management. Incident Response: Automated incident response processes. Key Outcomes:\nUnderstood the Shared Responsibility Model and common security threats. Learned how to establish multi-layer security architecture (Defense in Depth) and automate detection and response to security incidents on the Cloud. "},{"uri":"https://dangkhoavodong.github.io/fcj-workshop/4-eventparticipated/4.4-event4/","title":"Event 4","tags":[],"description":"","content":"Event Report: “Workshop: DevOps on AWS” Event Objectives Understand the true DevOps Mindset and Culture. Master the AWS CI/CD toolset to automate the software development process. Learn how to manage Infrastructure as Code (IaC). Get familiar with Container technology and modern application deployment strategies. Establish comprehensive Monitoring and Observability systems. List of Speakers AWS Technical Experts Team. Key Highlights This was a full-day workshop with in-depth knowledge, consisting of two main sessions:\n1. Morning Session: CI/CD \u0026amp; Infrastructure as Code DevOps Mindset: DevOps is not just about tools but a combination of culture, processes, and tools. Introduction to efficiency metrics (DORA metrics) such as deployment frequency and Mean Time to Recovery (MTTR). AWS CI/CD Pipeline: Source Control: Source code management with AWS CodeCommit and Git strategies (GitFlow, Trunk-based). Build \u0026amp; Test: Automating code building and testing with AWS CodeBuild. Deployment: Automating application deployment with AWS CodeDeploy (supporting Blue/Green, Canary). Orchestration: Coordinating the entire process using AWS CodePipeline. Infrastructure as Code (IaC):: CloudFormation: Creating and managing AWS resources using JSON/YAML templates, drift detection. AWS CDK: Defining infrastructure using familiar programming languages (Python, TypeScript, Java), making code reuse easier. 2. Afternoon Session: Containers \u0026amp; Observability Container Services: Amazon ECR: Secure Docker image storage. Amazon ECS \u0026amp; EKS: Comparison of container orchestration services, scaling strategies. AWS App Runner: Simplified container deployment solution for developers. Monitoring \u0026amp; Observability: CloudWatch: Collecting metrics, logs, creating dashboards, and alarms. AWS X-Ray: Tracing requests through distributed systems to find bottlenecks. DevOps Best Practices: Safe deployment strategies (Feature flags, A/B testing) and Incident management. Key Learnings Technical Knowledge Automation is King: Understood the power of automating everything, from committing code to deploying to production, helping to minimize human error. IaC vs. Click-ops: Clearly recognized the superiority of using code to manage infrastructure (CDK/CloudFormation) versus manual operations on the Console (Click-ops), especially regarding environment reproducibility. Observability: Distinguished between Monitoring (Is the system working?) and Observability (Why is the system working that way?), and the crucial role of X-Ray in microservices. Practical Skills Learned how to set up a basic CI/CD pipeline. Learned how to write a simple CDK script to create an S3 bucket or EC2 instance. Application to Work Project Improvement: Immediately apply AWS CodePipeline to the current internship project to automate code deployment whenever there is a new update. Switch to IaC: Start writing CloudFormation/CDK for currently used resources instead of creating them manually, facilitating configuration sharing with other team members. Optimize Monitoring: Set up CloudWatch Alarms to receive alerts via email/Slack when server CPU or Memory increases abnormally. Event Experience The DevOps on AWS workshop was a high-intensity learning day but extremely valuable.\nDepth and Flow The program went from Mindset to Tools and Practice, creating a very logical flow of knowledge. The Blue/Green Deployment demos helped me \u0026ldquo;see and hear\u0026rdquo; how to update applications with zero downtime - something I had only read about in theory before. Career Orientation Opportunity The Q\u0026amp;A session at the end regarding DevOps career pathways and the AWS certification roadmap helped me visualize the career ladder of a DevOps Engineer more clearly. Learning Environment Studying at the AWS office with full facilities and direct support from experts made absorbing difficult concepts like Kubernetes (EKS) or Tracing much easier. Key Takeaways DevOps is a journey of Continuous Improvement. Learning tools is necessary, but the mindset of automation and measurement is the core. Event Photos Insert screenshots of the CI/CD pipeline, architecture diagrams on the board, or photos with speakers here * Overall: The event helped me \u0026ldquo;upgrade\u0026rdquo; from a simple programmer mindset to that of a modern system engineer who knows how to master the product operation process.\n"},{"uri":"https://dangkhoavodong.github.io/fcj-workshop/5-workshop/5.4-s3-onprem/5.4.4-dns-simulation/","title":"On-premises DNS Simulation","tags":[],"description":"","content":"AWS PrivateLink endpoints have a fixed IP address in each Availability Zone where they are deployed, for the life of the endpoint (until it is deleted). These IP addresses are attached to Elastic Network Interfaces (ENIs). AWS recommends using DNS to resolve the IP addresses for endpoints so that downstream applications use the latest IP addresses when ENIs are added to new AZs, or deleted over time.\nIn this section, you will create a forwarding rule to send DNS resolution requests from a simulated on-premises environment to a Route 53 Private Hosted Zone. This section leverages the infrastructure deployed by CloudFormation in the Prepare the environment section.\nCreate DNS Alias Records for the Interface endpoint Navigate to the Route 53 management console (Hosted Zones section). The CloudFormation template you deployed in the Prepare the environment section created this Private Hosted Zone. Click on the name of the Private Hosted Zone, s3.us-east-1.amazonaws.com: Create a new record in the Private Hosted Zone: Record name and record type keep default options Alias Button: Click to enable Route traffic to: Alias to VPC Endpoint Region: US East (N. Virginia) [us-east-1] Choose endpoint: Paste the Regional VPC Endpoint DNS name from your text editor (you saved when doing section 4.3) Click Add another record, and add a second record using the following values. Click Create records when finished to create both records. Record name: *. Record type: keep default value (type A) Alias Button: Click to enable Route traffic to: Alias to VPC Endpoint Region: US East (N. Virginia) [us-east-1] Choose endpoint: Paste the Regional VPC Endpoint DNS name from your text editor The new records appear in the Route 53 console:\nCreate a Resolver Forwarding Rule Route 53 Resolver Forwarding Rules allow you to forward DNS queries from your VPC to other sources for name resolution. Outside of a workshop environment, you might use this feature to forward DNS queries from your VPC to DNS servers running on-premises. In this section, you will simulate an on-premises conditional forwarder by creating a forwarding rule that forwards DNS queries for Amazon S3 to a Private Hosted Zone running in \u0026ldquo;VPC Cloud\u0026rdquo; in-order to resolve the PrivateLink interface endpoint regional DNS name.\nFrom the Route 53 management console, click Inbound endpoints on the left side bar In the Inbound endpoints console, click the ID of the inbound endpoint Copy the two IP addresses listed to your text editor From the Route 53 menu, choose Resolver \u0026gt; Rules, and click Create rule: In the Create rule console: Name: myS3Rule Rule type: Forward Domain name: s3.us-east-1.amazonaws.com VPC: VPC On-prem Outbound endpoint: VPCOnpremOutboundEndpoint Target IP Addresses: Enter both IP addresses from your text editor (inbound endpoint addresses) and then click Submit You have successfully created resolver forwarding rule.\nTest the on-premises DNS Simulation Connect to Test-Interface-Endpoint EC2 instance with Session manager Test DNS resolution. The dig command will return the IP addresses assigned to the VPC Interface endpoint running in VPC Cloud (your IP\u0026rsquo;s will be different): dig +short s3.us-east-1.amazonaws.com The IP addresses returned are the VPC endpoint IP addresses, NOT the Resolver IP addresses you pasted from your text editor. The IP addresses of the Resolver endpoint and the VPC endpoint look similar because they are all from the VPC Cloud CIDR block.\nNavigate to the VPC menu (Endpoints section), select the S3 Interface endpoint. Click the Subnets tab and verify that the IP addresses returned by Dig match the VPC endpoint: Return to your shell and use the AWS CLI to test listing your S3 buckets: aws s3 ls --endpoint-url https://s3.us-east-1.amazonaws.com Terminate your Session Manager session: In this section you created an Interface endpoint for Amazon S3. This endpoint can be reached from on-premises through Site-to-Site VPN or AWS Direct Connect. Route 53 Resolver outbound endpoints simulated forwarding DNS requests from on-premises to a Private Hosted Zone running the cloud. Route 53 inbound Endpoints recieved the resolution request and returned a response containing the IP addresses of the VPC interface endpoint. Using DNS to resolve the endpoint IP addresses provides high availability in-case of an Availability Zone outage.\n"},{"uri":"https://dangkhoavodong.github.io/fcj-workshop/1-worklog/1.4-week4/","title":"Week 4 Worklog","tags":[],"description":"","content":"Week 4 Objectives: Learn more about S3 and its features. Start with IAM (Identity and Access Management). Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2-4 - Learned about S3 versioning - Configured S3 lifecycle policies - Practiced with S3 static website hosting - Learned about S3 encryption 09/29/2025 10/01/2025 AWS Journey 5-6 - Learned about IAM basics - Created IAM users, groups, roles - Configured IAM policies - Practiced with MFA 10/02/2025 10/03/2025 AWS Journey Week 4 Achievements: Mastered S3 features:\nConfigured S3 versioning to manage file versions Set up lifecycle policies to automatically transition storage class Successfully deployed static website on S3 Understand S3 encryption (SSE-S3, SSE-KMS) Configured CORS for S3 bucket Understand IAM and AWS security:\nGrasped what IAM is and its importance Created and managed IAM users Created IAM groups and assigned users to groups Created IAM roles for EC2 Wrote and applied IAM policies Configured MFA for root account and IAM users "},{"uri":"https://dangkhoavodong.github.io/fcj-workshop/4-eventparticipated/4.5-event5/","title":"Event 5","tags":[],"description":"","content":"Event Report: “Workshop: AWS Well-Architected Security Pillar” Event Objectives Understand the importance of the Security Pillar within the AWS Well-Architected Framework. Master the Shared Responsibility Model and core principles: Least Privilege, Zero Trust, Defense in Depth. Learn how to apply the 5 key security domains: IAM, Detection, Infrastructure Protection, Data Protection, and Incident Response. Identify common security threats in Vietnam and how to prevent them. List of Speakers AWS Security Specialists Team. Key Highlights The workshop focused deeply on the 5 main domains (5 Pillars) of Cloud security:\n1. Identity \u0026amp; Access Management (IAM) Principle: Avoid using long-term credentials for users. Solutions: Use IAM Roles instead of fixed Access Keys. IAM Identity Center: Centralized Single Sign-On (SSO) management. SCP (Service Control Policies): Establish security guardrails for multi-account environments. Enforce MFA and periodic credential rotation. 2. Detection Comprehensive Logging: Enable CloudTrail (organization-level), VPC Flow Logs, and S3 access logs. Detection Tools: Amazon GuardDuty: Detect anomalous behavior using Machine Learning. AWS Security Hub: Centralized security posture management. Detection-as-Code: Automate alert rule creation via infrastructure code. 3. Infrastructure Protection Network Security: VPC segmentation, placing resources in private subnets if public access is not required. Defense in Depth: Combine Security Groups (instance-level firewall), NACLs (subnet-level firewall), AWS WAF (Web App Firewall), and AWS Shield (DDoS protection). 4. Data Protection Encryption: Encrypt data both at-rest and in-transit. Key Management: Use AWS KMS to manage encryption keys and establish key rotation policies. Secrets Management: Do not hard-code passwords in code; use AWS Secrets Manager to store and automatically rotate DB credentials. 5. Incident Response (IR) IR Lifecycle: Preparation -\u0026gt; Detection -\u0026gt; Containment -\u0026gt; Investigation -\u0026gt; Recovery. Automation: Use AWS Lambda and Step Functions to automate response actions (e.g., automatically revoking permissions of exposed IAM users, isolating infected EC2 instances). Key Learnings Security Mindset Security is Day 0: Security is not the final step before deployment but must be integrated from the design phase (Shift Left Security). Zero Trust: Do not trust any user or device; always authenticate and authorize every request. Automate Security: Humans can make mistakes, but code is consistent. Automate as many security processes as possible. Technical Knowledge Clearly distinguished the difference and coordination between Security Groups (stateful) and NACLs (stateless). Understood how KMS Envelope Encryption works to protect data effectively. Application to Work Project Audit: Immediately check Security Groups in the internship project to remove unnecessary 0.0.0.0/0 rules (especially for SSH/RDP ports). Protect Secrets: Move all database credentials currently in configuration files to AWS Secrets Manager. Enable GuardDuty: Propose enabling GuardDuty to monitor suspicious behaviors in the team\u0026rsquo;s AWS account. Event Experience The Well-Architected Security Pillar workshop completely changed my perspective on security.\n\u0026ldquo;Startled\u0026rdquo; by Real-world Threats The sharing session on Top threats in Vietnam made me realize that exposing Access Keys or leaving S3 buckets public are elementary mistakes but have extremely serious consequences. Visual Demos Watching a live demo of how a hacker exploits an IAM vulnerability and how the system automatically detects and blocks it using Lambda was truly convincing. It proved the power of Automated Incident Response. Standardized Process I learned that incident response is not about \u0026ldquo;struggling\u0026rdquo; to fix errors but requires a clear Playbook: isolate, collect evidence (forensics), and then recover. Key Takeaways Security is everyone\u0026rsquo;s responsibility (Shared Responsibility), not just the Security team\u0026rsquo;s. As a Developer, writing secure code and configuring infrastructure correctly is the biggest contribution to the organization\u0026rsquo;s security. Event Photos Insert photos of slides about the Shared Responsibility Model, GuardDuty demo, or the workshop space here * Overall: The event helped me build a solid foundation to become an engineer who is not only skilled in features but also trustworthy in security (Security-first Engineer).\n"},{"uri":"https://dangkhoavodong.github.io/fcj-workshop/5-workshop/5.5-policy/","title":"VPC Endpoint Policies","tags":[],"description":"","content":"When you create an interface or gateway endpoint, you can attach an endpoint policy to it that controls access to the service to which you are connecting. A VPC endpoint policy is an IAM resource policy that you attach to an endpoint. If you do not attach a policy when you create an endpoint, AWS attaches a default policy for you that allows full access to the service through the endpoint.\nYou can create a policy that restricts access to specific S3 buckets only. This is useful if you only want certain S3 Buckets to be accessible through the endpoint.\nIn this section you will create a VPC endpoint policy that restricts access to the S3 bucket specified in the VPC endpoint policy.\nConnect to an EC2 instance and verify connectivity to S3 Start a new AWS Session Manager session on the instance named Test-Gateway-Endpoint. From the session, verify that you can list the contents of the bucket you created in Part 1: Access S3 from VPC: aws s3 ls s3://\\\u0026lt;your-bucket-name\\\u0026gt; The bucket contents include the two 1 GB files uploaded in earlier.\nCreate a new S3 bucket; follow the naming pattern you used in Part 1, but add a \u0026lsquo;-2\u0026rsquo; to the name. Leave other fields as default and click create Successfully create bucket\nNavigate to: Services \u0026gt; VPC \u0026gt; Endpoints, then select the Gateway VPC endpoint you created earlier. Click the Policy tab. Click Edit policy. The default policy allows access to all S3 Buckets through the VPC endpoint.\nIn Edit Policy console, copy \u0026amp; Paste the following policy, then replace yourbucketname-2 with your 2nd bucket name. This policy will allow access through the VPC endpoint to your new bucket, but not any other bucket in Amazon S3. Click Save to apply the policy. { \u0026#34;Id\u0026#34;: \u0026#34;Policy1631305502445\u0026#34;, \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;Stmt1631305501021\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;s3:*\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::yourbucketname-2\u0026#34;, \u0026#34;arn:aws:s3:::yourbucketname-2/*\u0026#34; ], \u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34; } ] } Successfully customize policy\nFrom your session on the Test-Gateway-Endpoint instance, test access to the S3 bucket you created in Part 1: Access S3 from VPC aws s3 ls s3://\u0026lt;yourbucketname\u0026gt; This command will return an error because access to this bucket is not permitted by your new VPC endpoint policy:\nReturn to your home directory on your EC2 instance cd~ Create a file fallocate -l 1G test-bucket2.xyz Copy file to 2nd bucket aws s3 cp test-bucket2.xyz s3://\u0026lt;your-2nd-bucket-name\u0026gt; This operation succeeds because it is permitted by the VPC endpoint policy.\nThen we test access to the first bucket by copy the file to 1st bucket aws s3 cp test-bucket2.xyz s3://\u0026lt;your-1st-bucket-name\u0026gt; This command will return an error because access to this bucket is not permitted by your new VPC endpoint policy.\nPart 3 Summary: In this section, you created a VPC endpoint policy for Amazon S3, and used the AWS CLI to test the policy. AWS CLI actions targeted to your original S3 bucket failed because you applied a policy that only allowed access to the second bucket you created. AWS CLI actions targeted for your second bucket succeeded because the policy allowed them. These policies can be useful in situations where you need to control access to resources through VPC endpoints.\n"},{"uri":"https://dangkhoavodong.github.io/fcj-workshop/1-worklog/1.5-week5/","title":"Week 5 Worklog","tags":[],"description":"","content":"Week 5 Objectives: Learn about VPC (Virtual Private Cloud). Practice basic networking on AWS. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2-3 - Learned about VPC basics - Understood CIDR blocks, subnets - Learned about public and private subnets 10/06/2025 10/07/2025 AWS Journey 4-5 - Created first VPC - Created public and private subnets - Configured Internet Gateway - Configured Route Tables 10/08/2025 10/09/2025 AWS Journey 6 - Learned about Security Groups and NACLs - Deployed EC2 in newly created VPC 10/10/2025 10/10/2025 AWS Journey Week 5 Achievements: Grasped VPC knowledge:\nUnderstand what VPC is and why we need VPC Grasped CIDR notation and how to calculate subnets Can distinguish public and private subnets Understand Internet Gateway and NAT Gateway Practiced creating VPC:\nCreated VPC with custom CIDR block Created public subnet and private subnet Configured Internet Gateway Set up Route Tables for subnets Deployed EC2 instances in VPC Understand network security:\nDistinguish Security Groups and NACLs Configured Security Groups for EC2 Set up inbound and outbound rules Understand stateful vs stateless firewalls "},{"uri":"https://dangkhoavodong.github.io/fcj-workshop/5-workshop/","title":"Workshop","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nSecure Hybrid Access to S3 using VPC Endpoints Overview AWS PrivateLink provides private connectivity to AWS services from VPCs and your on-premises networks, without exposing your traffic to the Public Internet.\nIn this lab, you will learn how to create, configure, and test VPC endpoints that enable your workloads to reach AWS services without traversing the Public Internet.\nYou will create two types of endpoints to access Amazon S3: a Gateway VPC endpoint, and an Interface VPC endpoint. These two types of VPC endpoints offer different benefits depending on if you are accessing Amazon S3 from the cloud or your on-premises location\nGateway - Create a gateway endpoint to send traffic to Amazon S3 or DynamoDB using private IP addresses.You route traffic from your VPC to the gateway endpoint using route tables. Interface - Create an interface endpoint to send traffic to endpoint services that use a Network Load Balancer to distribute traffic. Traffic destined for the endpoint service is resolved using DNS. Content Workshop overview Prerequiste Access S3 from VPC Access S3 from On-premises VPC Endpoint Policies (Bonus) Clean up "},{"uri":"https://dangkhoavodong.github.io/fcj-workshop/5-workshop/5.6-cleanup/","title":"Clean up","tags":[],"description":"","content":"Congratulations on completing this workshop! In this workshop, you learned architecture patterns for accessing Amazon S3 without using the Public Internet.\nBy creating a gateway endpoint, you enabled direct communication between EC2 resources and Amazon S3, without traversing an Internet Gateway. By creating an interface endpoint you extended S3 connectivity to resources running in your on-premises data center via AWS Site-to-Site VPN or Direct Connect. clean up Navigate to Hosted Zones on the left side of Route 53 console. Click the name of s3.us-east-1.amazonaws.com zone. Click Delete and confirm deletion by typing delete. Disassociate the Route 53 Resolver Rule - myS3Rule from \u0026ldquo;VPC Onprem\u0026rdquo; and Delete it. Open the CloudFormation console and delete the two CloudFormation Stacks that you created for this lab: PLOnpremSetup PLCloudSetup Delete S3 buckets Open S3 console Choose the bucket we created for the lab, click and confirm empty. Click delete and confirm delete. "},{"uri":"https://dangkhoavodong.github.io/fcj-workshop/6-self-evaluation/","title":"Self Assessment","tags":[],"description":"","content":"During my internship at First Cloud Journey from September 8, 2025 to December 12, 2025, I had the opportunity to learn, practice, and apply the knowledge equipped at university into a real-world working environment.\nI participated in researching and deploying solutions on the AWS platform, thereby improving my skills in researching new technologies, operating Cloud systems, and effective teamwork.\nRegarding work ethic, I always try to complete tasks well and actively communicate with colleagues to improve work efficiency. However, I realize that I need to be stricter with myself regarding discipline.\nTo objectively reflect on the internship process, I would like to self-assess based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional Knowledge \u0026amp; Skills Industry understanding, practical application, tool usage, work quality ✅ 2 Learning Ability Absorbing new knowledge, learning quickly ✅ 3 Proactivity Self-researching, taking on tasks without waiting for instructions ✅ 4 Responsibility Completing work on time, ensuring quality ✅ 5 Discipline Adhering to time, rules, and work processes ✅ 6 Progressiveness Willingness to receive feedback and improve oneself ✅ 7 Communication Presenting ideas and reporting work clearly ✅ 8 Teamwork Working effectively with colleagues, participating in group activities ✅ 9 Professional Conduct Respecting colleagues, partners, and the work environment ✅ 10 Problem-Solving Mindset Identifying problems, proposing solutions, creativity ✅ 11 Contribution to Project/Org Work efficiency, improvement initiatives, recognition from the team ✅ 12 Overall General assessment of the entire internship process ✅ Areas for Improvement Through self-reflection, I have identified areas that need to be addressed to improve myself in the future:\nEnhance Discipline: Strictly adhere to company and organizational regulations, especially regarding punctuality and administrative processes. Improve Problem-Solving Mindset: Practice viewing problems from multiple perspectives and proposing systematic, creative solutions rather than just addressing immediate symptoms. Communication Skills: Learn to express ideas more concisely and clearly during daily meetings, and practice calmness and tact when handling arising situations or conflicts. "},{"uri":"https://dangkhoavodong.github.io/fcj-workshop/1-worklog/1.6-week6/","title":"Week 6 Worklog","tags":[],"description":"","content":"Week 6 Objectives: Learn about Load Balancing. Start with RDS (Relational Database Service). Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2-4 - Learned about Elastic Load Balancer (ELB) - Load balancer types: ALB, NLB, CLB - Created Application Load Balancer - Configured target groups - Tested load balancing 10/13/2025 10/15/2025 AWS Journey 5-6 - Learned about Amazon RDS - Database engines (MySQL, PostgreSQL) - Created first RDS instance - Connected to RDS from EC2 10/16/2025 10/17/2025 AWS Journey Week 6 Achievements: Grasped Load Balancing:\nUnderstand what Load Balancer is and its benefits Distinguish ALB, NLB, CLB Successfully created Application Load Balancer Configured target groups and health checks Tested load balancing with multiple EC2 instances Understand sticky sessions and connection draining Started with RDS:\nUnderstand what RDS is and benefits vs self-managed database Know database engines AWS supports Successfully created RDS MySQL instance Configured security groups for RDS Connected from EC2 to RDS Understand RDS backups and snapshots "},{"uri":"https://dangkhoavodong.github.io/fcj-workshop/7-feedback/","title":"Sharing &amp; Feedback","tags":[],"description":"","content":" Here you can freely share your personal opinions about your experience participating in the First Cloud Journey program, helping the FCJ team improve based on the following categories:\nGeneral Assessment 1. Working Environment The environment at FCJ is extremely dynamic and professional. Participating in events at Bitexco or the AWS Vietnam office helped me clearly feel the culture of a top-tier tech company. Members embody the \u0026ldquo;Builder\u0026rdquo; spirit – always ready to build, experiment with new things, and unafraid to share knowledge.\n2. Support from Mentors / Team Admins Mentors are not just guides but true technical experts. I was impressed by how they explained complex concepts (like GenAI, DevOps mindset) in practical terms. The Team Admins were very enthusiastic in organizing workshops, ensuring sessions ran smoothly.\n3. Relevance between Work and Major The program closely aligns with and expands upon the knowledge I learned at university. While university taught cloud theory, here I saw how AWS is applied in real enterprises (Banking, Fintech, E-commerce). This is the perfect stepping stone for my Cloud Engineer career.\n4. Opportunities for Learning \u0026amp; Skill Development Learning opportunities are the highlight of the program. The \u0026ldquo;Cloud Mastery Series\u0026rdquo; workshops on AI/ML, Security, and DevOps contained high-value knowledge. I learned not only about tools (AWS Services) but also system design thinking and standardized workflows.\n5. Culture \u0026amp; Team Spirit The open culture is clearly evident. Even as an intern, my opinions were respected. Team spirit shone brightest during practical workshops, where everyone debugged errors together and shared resources.\n6. Policies / Benefits for Interns Beyond basic support, the biggest \u0026ldquo;benefit\u0026rdquo; I received was access to AWS learning resources, certification vouchers, and networking opportunities with industry leaders.\nOther Questions What were you most satisfied with during the internship? The professional quality of the Workshops and Community Day. I had access to the latest technologies (GenAI, Bedrock) right when they were trending hot in the market.\nWhat do you think the company needs to improve for future interns? I think the program could add \u0026ldquo;Code Camps\u0026rdquo; or mini-projects that last throughout the internship so interns have a chance for deeper hands-on practice, rather than just short-term workshops.\nWould you recommend this internship to your friends? Why? Definitely YES. This is the best environment to start a Cloud career in Vietnam. You don\u0026rsquo;t just learn a trade; you join a strong Community that will support you in the long run.\nSuggestions \u0026amp; Wishes Do you have any suggestions to improve the internship experience? I suggest increasing sharing sessions on soft skills, such as: Technical solution presentation skills, project documentation writing, or specialized English for IT.\nDo you want to continue with this program in the future? I strongly desire to continue accompanying FCJ in higher positions (such as Core Team or Mentor supporting future batches) or have the opportunity to become a full-time employee at partner companies within the ecosystem.\nOther feedback (free sharing): Thank you to the First Cloud Journey organizers for creating such a valuable playground. I hope the community will continue to grow and expand into even more in-depth topics.\n"},{"uri":"https://dangkhoavodong.github.io/fcj-workshop/1-worklog/1.7-week7/","title":"Week 7 Worklog","tags":[],"description":"","content":"Week 7 Objectives: Learn about Auto Scaling. Start with CloudWatch monitoring. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2-3 - Learned about EC2 Auto Scaling - Created Launch Template - Created Auto Scaling Group 10/20/2025 10/21/2025 AWS Journey 4-6 - Configured scaling policies - Learned about CloudWatch - Created CloudWatch alarms - Created CloudWatch dashboard 10/22/2025 10/24/2025 AWS Journey Week 7 Achievements: Grasped Auto Scaling:\nUnderstand what Auto Scaling is and its benefits Successfully created Launch Template Configured Auto Scaling Group Set up min/max/desired capacity Configured scaling policies Tested scaling out and scaling in Started with CloudWatch:\nUnderstand what CloudWatch is Viewed metrics of EC2, RDS Created CloudWatch alarms Configured SNS notifications Created dashboard to monitor resources "},{"uri":"https://dangkhoavodong.github.io/fcj-workshop/1-worklog/1.8-week8/","title":"Week 8 Worklog","tags":[],"description":"","content":"Week 8 Objectives: Learn about Lambda and serverless. Practice with basic CloudFormation. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2-4 - Learned about AWS Lambda - Created first Lambda function - Tested Lambda with events - Integrated Lambda with S3 10/27/2025 10/29/2025 AWS Journey 5-6 - Learned about Infrastructure as Code - Started with CloudFormation - Created simple CloudFormation template - Deployed stack with CloudFormation 10/30/2025 10/31/2025 AWS Journey Week 8 Achievements: Understand serverless and Lambda:\nGrasped what serverless architecture is Understand Lambda and use cases Created Lambda function with Python Tested Lambda function Integrated Lambda with S3 events Understand Lambda pricing Started with Infrastructure as Code:\nUnderstand what IaC is and its benefits Learned about CloudFormation Wrote CloudFormation template (YAML) Deployed EC2 and S3 using CloudFormation Updated and deleted stacks Understand stack rollback "},{"uri":"https://dangkhoavodong.github.io/fcj-workshop/1-worklog/1.9-week9/","title":"Week 9 Worklog","tags":[],"description":"","content":"Week 9 Objectives: Learn about Route 53 and DNS. Practice with CloudFront CDN. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2-3 - Learned about DNS and Route 53 - Registered domain (or used test domain) - Configured hosted zone - Created record types (A, CNAME) 11/03/2025 11/04/2025 AWS Journey 4-6 - Learned about CloudFront CDN - Created CloudFront distribution - Configured origin (S3) - Tested CDN performance 11/05/2025 11/07/2025 AWS Journey Week 9 Achievements: Grasped Route 53:\nUnderstand what DNS is and how it works Know record types (A, AAAA, CNAME, MX) Configured hosted zone Created records to point domain to resources Understand routing policies Tested DNS resolution Worked with CloudFront:\nUnderstand what CDN is and its benefits Created CloudFront distribution Configured S3 as origin Configured cache behaviors Tested performance improvement Understand edge locations "},{"uri":"https://dangkhoavodong.github.io/fcj-workshop/1-worklog/1.10-week10/","title":"Week 10 Worklog","tags":[],"description":"","content":"Week 10 Objectives: Learn about DynamoDB. Practice with SNS and SQS. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2-4 - Learned about DynamoDB (NoSQL database) - Created DynamoDB table - Practiced CRUD operations - Learned about partition key and sort key 11/10/2025 11/12/2025 AWS Journey 5-6 - Learned about SNS (Simple Notification Service) - Learned about SQS (Simple Queue Service) - Created SNS topic and subscriptions - Created SQS queue - Tested messaging 11/13/2025 11/14/2025 AWS Journey Week 10 Achievements: Grasped DynamoDB:\nUnderstand NoSQL database and difference from SQL Successfully created DynamoDB table Performed CRUD operations Understand partition key and sort key Queried and scanned data Understand DynamoDB pricing Worked with messaging services:\nUnderstand what SNS is (pub/sub model) Created SNS topic and subscriptions Sent notifications via email/SMS Understand what SQS is (queue model) Created SQS queue Sent and received messages Understand standard vs FIFO queues "},{"uri":"https://dangkhoavodong.github.io/fcj-workshop/1-worklog/1.11-week11/","title":"Week 11 Worklog","tags":[],"description":"","content":"Week 11 Objectives: Learn about ECS and container services. Practice with AWS best practices. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2-5 - Learned about Docker containers - Learned about ECS (Elastic Container Service) - Created Docker image - Pushed image to ECR - Deployed container on ECS 11/17/2025 11/20/2025 AWS Journey 6 - Reviewed AWS best practices - Learned about Well-Architected Framework - Reviewed security best practices 11/21/2025 11/21/2025 AWS Journey Week 11 Achievements: Understand containers and ECS:\nGrasped what Docker containers are Understand benefits of containerization Created Dockerfile Built Docker image Pushed image to ECR (Elastic Container Registry) Deployed container on ECS Understand ECS tasks and services Grasped AWS best practices:\nUnderstand 5 pillars of Well-Architected Framework Reviewed security best practices Understand cost optimization Know how to monitor and logging Grasped disaster recovery strategies "},{"uri":"https://dangkhoavodong.github.io/fcj-workshop/1-worklog/1.12-week12/","title":"Week 12 Worklog","tags":[],"description":"","content":"Week 12 Objectives: Build comprehensive project. Summarize and prepare final report. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2-4 - Designed architecture for comprehensive project - Deployed web application with: + EC2 + Auto Scaling + Load Balancer + RDS database + S3 for static files + CloudFront CDN 11/24/2025 11/26/2025 AWS Journey 5-6 - Completed project - Wrote architecture documentation - Prepared presentation - Summarized 12 weeks of learning 11/27/2025 11/28/2025 AWS Journey Week 12 Achievements: Completed comprehensive project:\nDesigned 3-tier application architecture Deployed complete web application Used Auto Scaling for high availability Configured Load Balancer Connected RDS database Used S3 and CloudFront for static content Implemented monitoring with CloudWatch Applied security best practices Summarized internship program:\nCompleted 12 weeks of AWS learning Mastered AWS core services Able to design and deploy applications on AWS Understand cloud architecture patterns Ready for AWS certification Wrote final report and presentation "},{"uri":"https://dangkhoavodong.github.io/fcj-workshop/1-worklog/1.13-week13/","title":"Week 13 Worklog","tags":[],"description":"","content":"Week 13 Objectives: Complete documentation and presentation. Present internship results. Summarize and evaluate the program. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2-3 - Completed technical documentation for project - Wrote user guide and deployment guide - Prepared presentation slides - Reviewed all learned knowledge 12/01/2025 12/02/2025 AWS Journey 4 - Presented to the team - Demoed comprehensive project - Answered questions and received feedback 12/03/2025 12/03/2025 AWS Journey 5-6 - Made revisions based on feedback - Wrote final internship report - Self-evaluation and development plan - Cleaned up AWS resources to avoid charges 12/04/2025 12/05/2025 AWS Journey Week 13 Achievements: Completed project documentation:\nWrote comprehensive architecture documentation Created detailed deployment guide Wrote user manual for application Prepared professional presentation slides Created demo video for project Successfully presented:\nPresented project to team and mentors Demoed application features Explained architecture and design decisions Answered technical questions Received positive feedback from team Summarized internship program:\nCompleted 13 weeks of internship at FCJ Mastered 20+ AWS core services Able to design and deploy applications on AWS Understand cloud architecture and best practices Ready for AWS Solutions Architect certification Built portfolio with real-world project Next steps:\nPrepare for AWS Solutions Architect Associate exam Continue learning advanced AWS services Participate in AWS community events Apply knowledge to real-world projects "},{"uri":"https://dangkhoavodong.github.io/fcj-workshop/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://dangkhoavodong.github.io/fcj-workshop/tags/","title":"Tags","tags":[],"description":"","content":""}]